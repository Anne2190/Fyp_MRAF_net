# MRAF-Net Configuration File
# Author: Anne Nidhusha Nithiyalan (w1985740)

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
data:
  # Path to BraTS dataset
  data_dir: "data/brats2020/MICCAI_BraTS2020_TrainingData"
  
  # MRI Modalities (order matters!)
  modalities:
    - "flair"
    - "t1"
    - "t1ce"
    - "t2"
  
  # Number of input channels (4 modalities)
  in_channels: 4
  
  # Number of output classes
  # 0: Background, 1: Necrotic/Core, 2: Edema, 3: Enhancing Tumor
  num_classes: 4
  
  # Train/Val split ratio
  train_val_split: 0.8
  
  # Random seed for reproducibility
  seed: 42

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model:
  # Model name
  name: "MRAF-Net"
  
  # Base number of features (channels)
  base_features: 32
  
  # Encoder feature multipliers at each level
  # Level 1: 32, Level 2: 64, Level 3: 128, Level 4: 256, Level 5: 320
  feature_multipliers: [1, 2, 4, 8, 10]
  
  # Use attention gates in decoder
  use_attention: true
  
  # Use cross-modality fusion
  use_cross_modal_fusion: true
  
  # Use ASPP module
  use_aspp: true
  
  # ASPP dilation rates
  aspp_dilations: [6, 12, 18]
  
  # Dropout rate
  dropout: 0.1
  
  # Use deep supervision
  deep_supervision: true

# =============================================================================
# TRAINING CONFIGURATION (LAPTOP MODE - Memory Optimized)
# =============================================================================
training:
  # Batch size (1-2 for laptops with 8GB VRAM)
  batch_size: 1
  
  # Number of training epochs
  epochs: 3   # 300
  
  # Initial learning rate
  learning_rate: 0.0001
  
  # Weight decay for regularization
  weight_decay: 0.00001
  
  # Optimizer: "adam", "adamw", "sgd"
  optimizer: "adamw"
  
  # Learning rate scheduler: "cosine", "step", "plateau"
  scheduler: "cosine"
  
  # Warmup epochs
  warmup_epochs: 10
  
  # Patch size for training (reduce if OOM)
  # Options: [128, 128, 128], [96, 96, 96], [64, 64, 64]
  patch_size: [96, 96, 96]
  
  # Number of patches per volume during training
  samples_per_volume: 4
  
  # Use Automatic Mixed Precision (AMP) for faster training
  use_amp: true
  
  # Use gradient checkpointing (saves memory, slower training)
  gradient_checkpointing: true
  
  # Gradient clipping max norm
  gradient_clip: 1.0
  
  # Number of data loader workers
  num_workers: 4
  
  # Pin memory for faster data transfer to GPU
  pin_memory: true
  
  # Validation frequency (epochs)
  val_frequency: 5
  
  # Early stopping patience (0 to disable)
  early_stopping_patience: 50

# =============================================================================
# LOSS CONFIGURATION
# =============================================================================
loss:
  # Loss function: "dice_ce", "dice", "focal", "dice_focal"
  name: "dice_ce"
  
  # Dice loss weight
  dice_weight: 1.0
  
  # Cross entropy weight
  ce_weight: 1.0
  
  # Focal loss gamma (if using focal)
  focal_gamma: 2.0
  
  # Deep supervision weights (for each decoder level)
  ds_weights: [1.0, 0.5, 0.25, 0.125]

# =============================================================================
# DATA AUGMENTATION
# =============================================================================
augmentation:
  # Enable augmentation during training
  enabled: true
  
  # Random flip probability
  flip_prob: 0.5
  
  # Random rotation (90 degree) probability
  rotate_prob: 0.5
  
  # Random scale intensity probability
  scale_intensity_prob: 0.5
  scale_intensity_range: [-0.1, 0.1]
  
  # Random shift intensity probability
  shift_intensity_prob: 0.5
  shift_intensity_range: [-0.1, 0.1]
  
  # Random Gaussian noise probability
  noise_prob: 0.2
  noise_std: 0.1
  
  # Random Gaussian blur probability
  blur_prob: 0.2

# =============================================================================
# INFERENCE CONFIGURATION
# =============================================================================
inference:
  # Sliding window inference ROI size
  roi_size: [128, 128, 128]
  
  # Sliding window batch size
  sw_batch_size: 4
  
  # Overlap ratio for sliding window
  overlap: 0.5
  
  # Inference mode: "gaussian", "constant"
  mode: "gaussian"
  
  # Test time augmentation (flip augmentation)
  use_tta: true

# =============================================================================
# CHECKPOINT AND LOGGING
# =============================================================================
checkpoint:
  # Directory to save checkpoints
  save_dir: "checkpoints"
  
  # Save checkpoint every N epochs
  save_frequency: 10
  
  # Keep only best N checkpoints
  keep_best: 3
  
  # Resume from checkpoint (set path or null)
  resume: null

logging:
  # Logging directory
  log_dir: "logs"
  
  # Use TensorBoard
  use_tensorboard: true
  
  # Print frequency (iterations)
  print_frequency: 10

# =============================================================================
# EVALUATION METRICS
# =============================================================================
metrics:
  # Metrics to compute
  compute:
    - "dice"
    - "hausdorff95"
    - "sensitivity"
    - "specificity"
  
  # Include background class in metrics
  include_background: false

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================
output:
  # Output directory for predictions
  output_dir: "outputs"
  
  # Save predictions as NIfTI files
  save_nifti: true
  
  # Save visualization images
  save_visualization: true
